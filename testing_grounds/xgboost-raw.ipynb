{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('ItemInfo_trainfull.csv', encoding='utf-8', nrows=15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Null values are numerical, impute with zero\n",
    "train = train.fillna(value=0,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "itemID                 0\n",
       "categoryID_x           0\n",
       "title_x                0\n",
       "description_x          0\n",
       "images_array_x         0\n",
       "attrsJSON_x            0\n",
       "price_x                0\n",
       "locationID_x           0\n",
       "metroID_x              0\n",
       "lat_x                  0\n",
       "lon_x                  0\n",
       "itemID_2               0\n",
       "isDuplicate            0\n",
       "generationMethod       0\n",
       "categoryID_y           0\n",
       "title_y                0\n",
       "description_y          0\n",
       "images_array_y         0\n",
       "attrsJSON_y            0\n",
       "price_y                0\n",
       "locationID_y           0\n",
       "metroID_y              0\n",
       "lat_y                  0\n",
       "lon_y                  0\n",
       "parentCategoryID_x     0\n",
       "parentCategoryID_y     0\n",
       "regionID_x             0\n",
       "regionID_y             0\n",
       "lendiff_imagearray     0\n",
       "priceDifference        0\n",
       "latlonDifference       0\n",
       "fuzz_ratio             0\n",
       "lev_dist               0\n",
       "jaro_dist              0\n",
       "jarow_dist             0\n",
       "description_x_clean    0\n",
       "description_y_clean    0\n",
       "intersect_BOW          0\n",
       "sym_diff_BOW           0\n",
       "clusters               0\n",
       "desc_sim               0\n",
       "City_x                 0\n",
       "Neighborhood_x         0\n",
       "State_x                0\n",
       "Street_x               0\n",
       "geocodeQuality_x       0\n",
       "postalCode_x           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nonnum_columns = [key for key in dict(train.dtypes) if dict(train.dtypes)[key] == 'object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "for feature in nonnum_columns:\n",
    "    train[feature] = le.fit_transform(train[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['isDuplicate']\n",
    "x = train.drop('isDuplicate', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train.isDuplicate[:34500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.drop('isDuplicate', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual split of training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_s = train.iloc[:34500,:]\n",
    "test_s = train.iloc[34500:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgtest = xgb.DMatrix(test_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10500"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_s.iloc[split:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split = 4500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgtrain = xgb.DMatrix(train_s.iloc[split:,:], label=labels[split:])\n",
    "xgval = xgb.DMatrix(train_s.iloc[:split,:], label=labels[:split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-error:0.28981\tval-error:0.335111\n",
      "Multiple eval metrics have been passed: 'val-error' will be used for early stopping.\n",
      "\n",
      "Will train until val-error hasn't improved in 80 rounds.\n",
      "[1]\ttrain-error:0.284\tval-error:0.310667\n",
      "[2]\ttrain-error:0.259714\tval-error:0.263556\n",
      "[3]\ttrain-error:0.246667\tval-error:0.260222\n",
      "[4]\ttrain-error:0.247619\tval-error:0.261778\n",
      "[5]\ttrain-error:0.249048\tval-error:0.260222\n",
      "[6]\ttrain-error:0.247905\tval-error:0.261556\n",
      "[7]\ttrain-error:0.243524\tval-error:0.257111\n",
      "[8]\ttrain-error:0.243048\tval-error:0.258\n",
      "[9]\ttrain-error:0.241524\tval-error:0.258222\n",
      "[10]\ttrain-error:0.242571\tval-error:0.258\n",
      "[11]\ttrain-error:0.243238\tval-error:0.258889\n",
      "[12]\ttrain-error:0.24\tval-error:0.255556\n",
      "[13]\ttrain-error:0.238571\tval-error:0.254889\n",
      "[14]\ttrain-error:0.237238\tval-error:0.254667\n",
      "[15]\ttrain-error:0.236286\tval-error:0.254889\n",
      "[16]\ttrain-error:0.232571\tval-error:0.249333\n",
      "[17]\ttrain-error:0.230952\tval-error:0.248\n",
      "[18]\ttrain-error:0.225048\tval-error:0.236667\n",
      "[19]\ttrain-error:0.222286\tval-error:0.238222\n",
      "[20]\ttrain-error:0.222095\tval-error:0.239556\n",
      "[21]\ttrain-error:0.22\tval-error:0.235111\n",
      "[22]\ttrain-error:0.219905\tval-error:0.233778\n",
      "[23]\ttrain-error:0.218571\tval-error:0.233556\n",
      "[24]\ttrain-error:0.21619\tval-error:0.236889\n",
      "[25]\ttrain-error:0.215429\tval-error:0.234444\n",
      "[26]\ttrain-error:0.213429\tval-error:0.234222\n",
      "[27]\ttrain-error:0.21219\tval-error:0.235111\n",
      "[28]\ttrain-error:0.21219\tval-error:0.234444\n",
      "[29]\ttrain-error:0.210476\tval-error:0.233111\n",
      "[30]\ttrain-error:0.210095\tval-error:0.231111\n",
      "[31]\ttrain-error:0.210095\tval-error:0.232222\n",
      "[32]\ttrain-error:0.208857\tval-error:0.232667\n",
      "[33]\ttrain-error:0.208286\tval-error:0.231333\n",
      "[34]\ttrain-error:0.205524\tval-error:0.230444\n",
      "[35]\ttrain-error:0.205714\tval-error:0.229556\n",
      "[36]\ttrain-error:0.205143\tval-error:0.229333\n",
      "[37]\ttrain-error:0.203905\tval-error:0.229333\n",
      "[38]\ttrain-error:0.201429\tval-error:0.228889\n",
      "[39]\ttrain-error:0.201143\tval-error:0.228889\n",
      "[40]\ttrain-error:0.2\tval-error:0.228889\n",
      "[41]\ttrain-error:0.199333\tval-error:0.229333\n",
      "[42]\ttrain-error:0.197333\tval-error:0.228444\n",
      "[43]\ttrain-error:0.19581\tval-error:0.228667\n",
      "[44]\ttrain-error:0.195429\tval-error:0.229556\n",
      "[45]\ttrain-error:0.195619\tval-error:0.227333\n",
      "[46]\ttrain-error:0.195333\tval-error:0.228\n",
      "[47]\ttrain-error:0.193524\tval-error:0.229778\n",
      "[48]\ttrain-error:0.193905\tval-error:0.227333\n",
      "[49]\ttrain-error:0.192952\tval-error:0.228222\n",
      "[50]\ttrain-error:0.191524\tval-error:0.225778\n",
      "[51]\ttrain-error:0.191429\tval-error:0.226\n",
      "[52]\ttrain-error:0.19\tval-error:0.225333\n",
      "[53]\ttrain-error:0.187905\tval-error:0.225111\n",
      "[54]\ttrain-error:0.185333\tval-error:0.225778\n",
      "[55]\ttrain-error:0.185238\tval-error:0.227333\n",
      "[56]\ttrain-error:0.18581\tval-error:0.228444\n",
      "[57]\ttrain-error:0.184762\tval-error:0.227556\n",
      "[58]\ttrain-error:0.185238\tval-error:0.225333\n",
      "[59]\ttrain-error:0.18419\tval-error:0.226222\n",
      "[60]\ttrain-error:0.183048\tval-error:0.227111\n",
      "[61]\ttrain-error:0.182286\tval-error:0.227333\n",
      "[62]\ttrain-error:0.18219\tval-error:0.227778\n",
      "[63]\ttrain-error:0.180762\tval-error:0.224889\n",
      "[64]\ttrain-error:0.180476\tval-error:0.224\n",
      "[65]\ttrain-error:0.178857\tval-error:0.223556\n",
      "[66]\ttrain-error:0.178381\tval-error:0.224667\n",
      "[67]\ttrain-error:0.178476\tval-error:0.224444\n",
      "[68]\ttrain-error:0.177714\tval-error:0.224889\n",
      "[69]\ttrain-error:0.178381\tval-error:0.224667\n",
      "[70]\ttrain-error:0.177429\tval-error:0.224222\n",
      "[71]\ttrain-error:0.177714\tval-error:0.224667\n",
      "[72]\ttrain-error:0.177619\tval-error:0.224444\n",
      "[73]\ttrain-error:0.176762\tval-error:0.224889\n",
      "[74]\ttrain-error:0.176857\tval-error:0.225111\n",
      "[75]\ttrain-error:0.175524\tval-error:0.226667\n",
      "[76]\ttrain-error:0.174667\tval-error:0.226667\n",
      "[77]\ttrain-error:0.174286\tval-error:0.226667\n",
      "[78]\ttrain-error:0.173238\tval-error:0.226\n",
      "[79]\ttrain-error:0.173524\tval-error:0.226222\n",
      "[80]\ttrain-error:0.173429\tval-error:0.226\n",
      "[81]\ttrain-error:0.173524\tval-error:0.225333\n",
      "[82]\ttrain-error:0.172952\tval-error:0.226\n",
      "[83]\ttrain-error:0.173429\tval-error:0.226222\n",
      "[84]\ttrain-error:0.173048\tval-error:0.225556\n",
      "[85]\ttrain-error:0.172476\tval-error:0.224444\n",
      "[86]\ttrain-error:0.172571\tval-error:0.225111\n",
      "[87]\ttrain-error:0.17181\tval-error:0.225556\n",
      "[88]\ttrain-error:0.172\tval-error:0.224667\n",
      "[89]\ttrain-error:0.172\tval-error:0.223778\n",
      "[90]\ttrain-error:0.169619\tval-error:0.224444\n",
      "[91]\ttrain-error:0.169905\tval-error:0.224667\n",
      "[92]\ttrain-error:0.16981\tval-error:0.224889\n",
      "[93]\ttrain-error:0.169429\tval-error:0.224\n",
      "[94]\ttrain-error:0.168952\tval-error:0.224667\n",
      "[95]\ttrain-error:0.169333\tval-error:0.224\n",
      "[96]\ttrain-error:0.169714\tval-error:0.223778\n",
      "[97]\ttrain-error:0.169429\tval-error:0.222\n",
      "[98]\ttrain-error:0.168952\tval-error:0.222222\n",
      "[99]\ttrain-error:0.169143\tval-error:0.222222\n"
     ]
    }
   ],
   "source": [
    "param = {'max_depth':3, 'eta':0.1, 'silent':1, 'objective':'binary:logistic' }\n",
    "watchlist = [(xgtrain, 'train'), (xgval, 'val')]\n",
    "num_round = 100\n",
    "bst = xgb.train(param, xgtrain, num_boost_round=100, evals=watchlist, early_stopping_rounds=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = bst.predict(xgtest)\n",
    "# len(preds)\n",
    "preds = [round(value) for value in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [5000, 0]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-227-e0853c5a044c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/juan/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.pyc\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mdiffering_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/juan/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.pyc\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/juan/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 181\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [5000, 0]"
     ]
    }
   ],
   "source": [
    "accuracy_score(y_test[:5000], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50375669827011327"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test[:5000], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_score': 0.5,\n",
       " 'colsample_bylevel': 1,\n",
       " 'colsample_bytree': 1,\n",
       " 'gamma': 0,\n",
       " 'learning_rate': 0.1,\n",
       " 'max_delta_step': 0,\n",
       " 'max_depth': 3,\n",
       " 'min_child_weight': 1,\n",
       " 'missing': None,\n",
       " 'n_estimators': 100,\n",
       " 'objective': 'binary:logistic',\n",
       " 'reg_alpha': 0,\n",
       " 'reg_lambda': 1,\n",
       " 'scale_pos_weight': 1,\n",
       " 'seed': 0,\n",
       " 'silent': 1,\n",
       " 'subsample': 1}"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.XGBClassifier().get_xgb_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_cv = xgb.cv(param, xgtrain, num_boost_round=1000, nfold=10, early_stopping_rounds=80, metrics=['auc', 'error'], stratified=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test-auc-mean</th>\n",
       "      <th>test-auc-std</th>\n",
       "      <th>test-error-mean</th>\n",
       "      <th>test-error-std</th>\n",
       "      <th>train-auc-mean</th>\n",
       "      <th>train-auc-std</th>\n",
       "      <th>train-error-mean</th>\n",
       "      <th>train-error-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.758794</td>\n",
       "      <td>0.015767</td>\n",
       "      <td>0.289825</td>\n",
       "      <td>0.018040</td>\n",
       "      <td>0.760361</td>\n",
       "      <td>0.001767</td>\n",
       "      <td>0.289050</td>\n",
       "      <td>0.001969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.802782</td>\n",
       "      <td>0.015924</td>\n",
       "      <td>0.267646</td>\n",
       "      <td>0.016321</td>\n",
       "      <td>0.808222</td>\n",
       "      <td>0.002354</td>\n",
       "      <td>0.262869</td>\n",
       "      <td>0.001761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.818572</td>\n",
       "      <td>0.017488</td>\n",
       "      <td>0.268917</td>\n",
       "      <td>0.019483</td>\n",
       "      <td>0.825510</td>\n",
       "      <td>0.002065</td>\n",
       "      <td>0.260303</td>\n",
       "      <td>0.004287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.822171</td>\n",
       "      <td>0.016184</td>\n",
       "      <td>0.267107</td>\n",
       "      <td>0.018521</td>\n",
       "      <td>0.835368</td>\n",
       "      <td>0.004684</td>\n",
       "      <td>0.253354</td>\n",
       "      <td>0.006308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.832685</td>\n",
       "      <td>0.017789</td>\n",
       "      <td>0.251652</td>\n",
       "      <td>0.019065</td>\n",
       "      <td>0.844515</td>\n",
       "      <td>0.004521</td>\n",
       "      <td>0.243656</td>\n",
       "      <td>0.006782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.842989</td>\n",
       "      <td>0.017593</td>\n",
       "      <td>0.244016</td>\n",
       "      <td>0.020183</td>\n",
       "      <td>0.855969</td>\n",
       "      <td>0.006981</td>\n",
       "      <td>0.232343</td>\n",
       "      <td>0.006827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.850755</td>\n",
       "      <td>0.018323</td>\n",
       "      <td>0.240185</td>\n",
       "      <td>0.020815</td>\n",
       "      <td>0.864187</td>\n",
       "      <td>0.006505</td>\n",
       "      <td>0.223656</td>\n",
       "      <td>0.007023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.856097</td>\n",
       "      <td>0.015921</td>\n",
       "      <td>0.232369</td>\n",
       "      <td>0.020996</td>\n",
       "      <td>0.872204</td>\n",
       "      <td>0.004592</td>\n",
       "      <td>0.215677</td>\n",
       "      <td>0.003877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.861884</td>\n",
       "      <td>0.018637</td>\n",
       "      <td>0.224370</td>\n",
       "      <td>0.021199</td>\n",
       "      <td>0.877873</td>\n",
       "      <td>0.003055</td>\n",
       "      <td>0.208222</td>\n",
       "      <td>0.003812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.864570</td>\n",
       "      <td>0.018169</td>\n",
       "      <td>0.223276</td>\n",
       "      <td>0.020401</td>\n",
       "      <td>0.882255</td>\n",
       "      <td>0.002456</td>\n",
       "      <td>0.204444</td>\n",
       "      <td>0.003035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.865710</td>\n",
       "      <td>0.016315</td>\n",
       "      <td>0.224363</td>\n",
       "      <td>0.014594</td>\n",
       "      <td>0.885655</td>\n",
       "      <td>0.002517</td>\n",
       "      <td>0.200505</td>\n",
       "      <td>0.004345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.863911</td>\n",
       "      <td>0.015141</td>\n",
       "      <td>0.225814</td>\n",
       "      <td>0.016588</td>\n",
       "      <td>0.888246</td>\n",
       "      <td>0.002642</td>\n",
       "      <td>0.199010</td>\n",
       "      <td>0.004680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.864450</td>\n",
       "      <td>0.015985</td>\n",
       "      <td>0.228353</td>\n",
       "      <td>0.015244</td>\n",
       "      <td>0.891158</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>0.194788</td>\n",
       "      <td>0.003562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.016432</td>\n",
       "      <td>0.225813</td>\n",
       "      <td>0.016279</td>\n",
       "      <td>0.893737</td>\n",
       "      <td>0.002349</td>\n",
       "      <td>0.191394</td>\n",
       "      <td>0.003716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.867591</td>\n",
       "      <td>0.016896</td>\n",
       "      <td>0.218719</td>\n",
       "      <td>0.017461</td>\n",
       "      <td>0.895889</td>\n",
       "      <td>0.002458</td>\n",
       "      <td>0.188646</td>\n",
       "      <td>0.004389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.869232</td>\n",
       "      <td>0.016502</td>\n",
       "      <td>0.216716</td>\n",
       "      <td>0.016262</td>\n",
       "      <td>0.897989</td>\n",
       "      <td>0.002259</td>\n",
       "      <td>0.185535</td>\n",
       "      <td>0.004705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.870152</td>\n",
       "      <td>0.015991</td>\n",
       "      <td>0.213080</td>\n",
       "      <td>0.014840</td>\n",
       "      <td>0.899890</td>\n",
       "      <td>0.002419</td>\n",
       "      <td>0.183555</td>\n",
       "      <td>0.003293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.869875</td>\n",
       "      <td>0.016161</td>\n",
       "      <td>0.213995</td>\n",
       "      <td>0.011090</td>\n",
       "      <td>0.902002</td>\n",
       "      <td>0.002341</td>\n",
       "      <td>0.182343</td>\n",
       "      <td>0.002967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.870036</td>\n",
       "      <td>0.016433</td>\n",
       "      <td>0.213459</td>\n",
       "      <td>0.014036</td>\n",
       "      <td>0.903801</td>\n",
       "      <td>0.002256</td>\n",
       "      <td>0.179454</td>\n",
       "      <td>0.002457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.870404</td>\n",
       "      <td>0.016668</td>\n",
       "      <td>0.212184</td>\n",
       "      <td>0.017445</td>\n",
       "      <td>0.905349</td>\n",
       "      <td>0.002333</td>\n",
       "      <td>0.178525</td>\n",
       "      <td>0.004018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.871198</td>\n",
       "      <td>0.018399</td>\n",
       "      <td>0.214550</td>\n",
       "      <td>0.019791</td>\n",
       "      <td>0.907072</td>\n",
       "      <td>0.002360</td>\n",
       "      <td>0.176283</td>\n",
       "      <td>0.003908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.872045</td>\n",
       "      <td>0.017199</td>\n",
       "      <td>0.215827</td>\n",
       "      <td>0.015160</td>\n",
       "      <td>0.908730</td>\n",
       "      <td>0.002665</td>\n",
       "      <td>0.174626</td>\n",
       "      <td>0.004488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.872986</td>\n",
       "      <td>0.016462</td>\n",
       "      <td>0.214914</td>\n",
       "      <td>0.015432</td>\n",
       "      <td>0.910028</td>\n",
       "      <td>0.002525</td>\n",
       "      <td>0.173071</td>\n",
       "      <td>0.004358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.874012</td>\n",
       "      <td>0.016891</td>\n",
       "      <td>0.213637</td>\n",
       "      <td>0.016025</td>\n",
       "      <td>0.911437</td>\n",
       "      <td>0.002543</td>\n",
       "      <td>0.171838</td>\n",
       "      <td>0.004405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.874083</td>\n",
       "      <td>0.016789</td>\n",
       "      <td>0.211273</td>\n",
       "      <td>0.016849</td>\n",
       "      <td>0.912912</td>\n",
       "      <td>0.002606</td>\n",
       "      <td>0.169414</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.874017</td>\n",
       "      <td>0.017202</td>\n",
       "      <td>0.209995</td>\n",
       "      <td>0.014722</td>\n",
       "      <td>0.914178</td>\n",
       "      <td>0.002549</td>\n",
       "      <td>0.167757</td>\n",
       "      <td>0.003140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.872389</td>\n",
       "      <td>0.016973</td>\n",
       "      <td>0.213819</td>\n",
       "      <td>0.014771</td>\n",
       "      <td>0.915385</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.166788</td>\n",
       "      <td>0.003074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.872636</td>\n",
       "      <td>0.016737</td>\n",
       "      <td>0.212365</td>\n",
       "      <td>0.017421</td>\n",
       "      <td>0.916596</td>\n",
       "      <td>0.002499</td>\n",
       "      <td>0.164767</td>\n",
       "      <td>0.003358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.874815</td>\n",
       "      <td>0.016704</td>\n",
       "      <td>0.212908</td>\n",
       "      <td>0.015836</td>\n",
       "      <td>0.918193</td>\n",
       "      <td>0.002669</td>\n",
       "      <td>0.163919</td>\n",
       "      <td>0.003733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.874125</td>\n",
       "      <td>0.017357</td>\n",
       "      <td>0.214729</td>\n",
       "      <td>0.016860</td>\n",
       "      <td>0.919350</td>\n",
       "      <td>0.002503</td>\n",
       "      <td>0.163091</td>\n",
       "      <td>0.003787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.874428</td>\n",
       "      <td>0.017151</td>\n",
       "      <td>0.213455</td>\n",
       "      <td>0.016341</td>\n",
       "      <td>0.920463</td>\n",
       "      <td>0.002535</td>\n",
       "      <td>0.161434</td>\n",
       "      <td>0.004660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.874374</td>\n",
       "      <td>0.016543</td>\n",
       "      <td>0.210359</td>\n",
       "      <td>0.017052</td>\n",
       "      <td>0.921573</td>\n",
       "      <td>0.002406</td>\n",
       "      <td>0.159818</td>\n",
       "      <td>0.004495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.873786</td>\n",
       "      <td>0.016779</td>\n",
       "      <td>0.212906</td>\n",
       "      <td>0.016167</td>\n",
       "      <td>0.922611</td>\n",
       "      <td>0.002275</td>\n",
       "      <td>0.158808</td>\n",
       "      <td>0.004053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.873903</td>\n",
       "      <td>0.016860</td>\n",
       "      <td>0.211269</td>\n",
       "      <td>0.019770</td>\n",
       "      <td>0.923706</td>\n",
       "      <td>0.002121</td>\n",
       "      <td>0.157353</td>\n",
       "      <td>0.004173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.874436</td>\n",
       "      <td>0.016372</td>\n",
       "      <td>0.212908</td>\n",
       "      <td>0.020492</td>\n",
       "      <td>0.924657</td>\n",
       "      <td>0.002046</td>\n",
       "      <td>0.156060</td>\n",
       "      <td>0.004169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.873803</td>\n",
       "      <td>0.016879</td>\n",
       "      <td>0.210364</td>\n",
       "      <td>0.016824</td>\n",
       "      <td>0.925664</td>\n",
       "      <td>0.001927</td>\n",
       "      <td>0.154262</td>\n",
       "      <td>0.004511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.873231</td>\n",
       "      <td>0.017014</td>\n",
       "      <td>0.212906</td>\n",
       "      <td>0.017823</td>\n",
       "      <td>0.926789</td>\n",
       "      <td>0.001964</td>\n",
       "      <td>0.153535</td>\n",
       "      <td>0.004583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.873566</td>\n",
       "      <td>0.017930</td>\n",
       "      <td>0.214364</td>\n",
       "      <td>0.019166</td>\n",
       "      <td>0.927820</td>\n",
       "      <td>0.001767</td>\n",
       "      <td>0.151798</td>\n",
       "      <td>0.004205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.874716</td>\n",
       "      <td>0.017944</td>\n",
       "      <td>0.210725</td>\n",
       "      <td>0.019177</td>\n",
       "      <td>0.928784</td>\n",
       "      <td>0.001660</td>\n",
       "      <td>0.150989</td>\n",
       "      <td>0.004132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.875773</td>\n",
       "      <td>0.017954</td>\n",
       "      <td>0.208722</td>\n",
       "      <td>0.018193</td>\n",
       "      <td>0.929644</td>\n",
       "      <td>0.001632</td>\n",
       "      <td>0.149838</td>\n",
       "      <td>0.003725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.875231</td>\n",
       "      <td>0.017252</td>\n",
       "      <td>0.210360</td>\n",
       "      <td>0.019668</td>\n",
       "      <td>0.930557</td>\n",
       "      <td>0.001656</td>\n",
       "      <td>0.148343</td>\n",
       "      <td>0.003468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.874895</td>\n",
       "      <td>0.017525</td>\n",
       "      <td>0.211449</td>\n",
       "      <td>0.019376</td>\n",
       "      <td>0.931597</td>\n",
       "      <td>0.001681</td>\n",
       "      <td>0.146808</td>\n",
       "      <td>0.003355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.875566</td>\n",
       "      <td>0.017462</td>\n",
       "      <td>0.211267</td>\n",
       "      <td>0.020788</td>\n",
       "      <td>0.932548</td>\n",
       "      <td>0.001584</td>\n",
       "      <td>0.146121</td>\n",
       "      <td>0.004018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.875434</td>\n",
       "      <td>0.016581</td>\n",
       "      <td>0.210541</td>\n",
       "      <td>0.018060</td>\n",
       "      <td>0.933431</td>\n",
       "      <td>0.001564</td>\n",
       "      <td>0.144727</td>\n",
       "      <td>0.004688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.875289</td>\n",
       "      <td>0.016913</td>\n",
       "      <td>0.210543</td>\n",
       "      <td>0.017740</td>\n",
       "      <td>0.934362</td>\n",
       "      <td>0.001526</td>\n",
       "      <td>0.144323</td>\n",
       "      <td>0.003882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.875219</td>\n",
       "      <td>0.016854</td>\n",
       "      <td>0.208365</td>\n",
       "      <td>0.014203</td>\n",
       "      <td>0.935131</td>\n",
       "      <td>0.001480</td>\n",
       "      <td>0.143252</td>\n",
       "      <td>0.004612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    test-auc-mean  test-auc-std  test-error-mean  test-error-std  \\\n",
       "0        0.758794      0.015767         0.289825        0.018040   \n",
       "1        0.802782      0.015924         0.267646        0.016321   \n",
       "2        0.818572      0.017488         0.268917        0.019483   \n",
       "3        0.822171      0.016184         0.267107        0.018521   \n",
       "4        0.832685      0.017789         0.251652        0.019065   \n",
       "5        0.842989      0.017593         0.244016        0.020183   \n",
       "6        0.850755      0.018323         0.240185        0.020815   \n",
       "7        0.856097      0.015921         0.232369        0.020996   \n",
       "8        0.861884      0.018637         0.224370        0.021199   \n",
       "9        0.864570      0.018169         0.223276        0.020401   \n",
       "10       0.865710      0.016315         0.224363        0.014594   \n",
       "11       0.863911      0.015141         0.225814        0.016588   \n",
       "12       0.864450      0.015985         0.228353        0.015244   \n",
       "13       0.865672      0.016432         0.225813        0.016279   \n",
       "14       0.867591      0.016896         0.218719        0.017461   \n",
       "15       0.869232      0.016502         0.216716        0.016262   \n",
       "16       0.870152      0.015991         0.213080        0.014840   \n",
       "17       0.869875      0.016161         0.213995        0.011090   \n",
       "18       0.870036      0.016433         0.213459        0.014036   \n",
       "19       0.870404      0.016668         0.212184        0.017445   \n",
       "20       0.871198      0.018399         0.214550        0.019791   \n",
       "21       0.872045      0.017199         0.215827        0.015160   \n",
       "22       0.872986      0.016462         0.214914        0.015432   \n",
       "23       0.874012      0.016891         0.213637        0.016025   \n",
       "24       0.874083      0.016789         0.211273        0.016849   \n",
       "25       0.874017      0.017202         0.209995        0.014722   \n",
       "26       0.872389      0.016973         0.213819        0.014771   \n",
       "27       0.872636      0.016737         0.212365        0.017421   \n",
       "28       0.874815      0.016704         0.212908        0.015836   \n",
       "29       0.874125      0.017357         0.214729        0.016860   \n",
       "30       0.874428      0.017151         0.213455        0.016341   \n",
       "31       0.874374      0.016543         0.210359        0.017052   \n",
       "32       0.873786      0.016779         0.212906        0.016167   \n",
       "33       0.873903      0.016860         0.211269        0.019770   \n",
       "34       0.874436      0.016372         0.212908        0.020492   \n",
       "35       0.873803      0.016879         0.210364        0.016824   \n",
       "36       0.873231      0.017014         0.212906        0.017823   \n",
       "37       0.873566      0.017930         0.214364        0.019166   \n",
       "38       0.874716      0.017944         0.210725        0.019177   \n",
       "39       0.875773      0.017954         0.208722        0.018193   \n",
       "40       0.875231      0.017252         0.210360        0.019668   \n",
       "41       0.874895      0.017525         0.211449        0.019376   \n",
       "42       0.875566      0.017462         0.211267        0.020788   \n",
       "43       0.875434      0.016581         0.210541        0.018060   \n",
       "44       0.875289      0.016913         0.210543        0.017740   \n",
       "45       0.875219      0.016854         0.208365        0.014203   \n",
       "\n",
       "    train-auc-mean  train-auc-std  train-error-mean  train-error-std  \n",
       "0         0.760361       0.001767          0.289050         0.001969  \n",
       "1         0.808222       0.002354          0.262869         0.001761  \n",
       "2         0.825510       0.002065          0.260303         0.004287  \n",
       "3         0.835368       0.004684          0.253354         0.006308  \n",
       "4         0.844515       0.004521          0.243656         0.006782  \n",
       "5         0.855969       0.006981          0.232343         0.006827  \n",
       "6         0.864187       0.006505          0.223656         0.007023  \n",
       "7         0.872204       0.004592          0.215677         0.003877  \n",
       "8         0.877873       0.003055          0.208222         0.003812  \n",
       "9         0.882255       0.002456          0.204444         0.003035  \n",
       "10        0.885655       0.002517          0.200505         0.004345  \n",
       "11        0.888246       0.002642          0.199010         0.004680  \n",
       "12        0.891158       0.002300          0.194788         0.003562  \n",
       "13        0.893737       0.002349          0.191394         0.003716  \n",
       "14        0.895889       0.002458          0.188646         0.004389  \n",
       "15        0.897989       0.002259          0.185535         0.004705  \n",
       "16        0.899890       0.002419          0.183555         0.003293  \n",
       "17        0.902002       0.002341          0.182343         0.002967  \n",
       "18        0.903801       0.002256          0.179454         0.002457  \n",
       "19        0.905349       0.002333          0.178525         0.004018  \n",
       "20        0.907072       0.002360          0.176283         0.003908  \n",
       "21        0.908730       0.002665          0.174626         0.004488  \n",
       "22        0.910028       0.002525          0.173071         0.004358  \n",
       "23        0.911437       0.002543          0.171838         0.004405  \n",
       "24        0.912912       0.002606          0.169414         0.004000  \n",
       "25        0.914178       0.002549          0.167757         0.003140  \n",
       "26        0.915385       0.002500          0.166788         0.003074  \n",
       "27        0.916596       0.002499          0.164767         0.003358  \n",
       "28        0.918193       0.002669          0.163919         0.003733  \n",
       "29        0.919350       0.002503          0.163091         0.003787  \n",
       "30        0.920463       0.002535          0.161434         0.004660  \n",
       "31        0.921573       0.002406          0.159818         0.004495  \n",
       "32        0.922611       0.002275          0.158808         0.004053  \n",
       "33        0.923706       0.002121          0.157353         0.004173  \n",
       "34        0.924657       0.002046          0.156060         0.004169  \n",
       "35        0.925664       0.001927          0.154262         0.004511  \n",
       "36        0.926789       0.001964          0.153535         0.004583  \n",
       "37        0.927820       0.001767          0.151798         0.004205  \n",
       "38        0.928784       0.001660          0.150989         0.004132  \n",
       "39        0.929644       0.001632          0.149838         0.003725  \n",
       "40        0.930557       0.001656          0.148343         0.003468  \n",
       "41        0.931597       0.001681          0.146808         0.003355  \n",
       "42        0.932548       0.001584          0.146121         0.004018  \n",
       "43        0.933431       0.001564          0.144727         0.004688  \n",
       "44        0.934362       0.001526          0.144323         0.003882  \n",
       "45        0.935131       0.001480          0.143252         0.004612  "
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75879380000000007"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst_cv['test-auc-mean'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 1\n",
    "test_size = 0.77\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = test_size, random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = pd.concat([x_train, y_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test1 = pd.concat([x_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xgb = xgb.DMatrix(train1,label=y_train)\n",
    "test_xgb = xgb.DMatrix(test1, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param = {'max_depth':2, 'eta':1, 'silent':1, 'objective':'binary:logistic' }\n",
    "num_round = 2\n",
    "watchlist = [(test_xgb, 'eval'), (train_xgb, 'train')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-error:0\ttrain-error:0\n",
      "[1]\teval-error:0\ttrain-error:0\n"
     ]
    }
   ],
   "source": [
    "bst = xgb.train(param, train_xgb, num_round, watchlist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
